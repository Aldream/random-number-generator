% This is based on the LLNCS.DEM the demonstration file of
% the LaTeX macro package from Springer-Verlag
% for Lecture Notes in Computer Science,
% version 2.4 for LaTeX2e as of 16. April 2010
%
% See http://www.springer.com/computer/lncs/lncs+authors?SGWID=0-40209-0-0-0
% for the full guidelines.
%
\documentclass{llncs}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{nth}
\usepackage[section]{algorithm}
\usepackage{algpseudocode}

\pagestyle{headings}
\begin{document}

\title{Survey on Random Number Generators}
%
\titlerunning{RNG Survey}  % abbreviated title (for running head)
%                                     also used for the TOC unless
%                                     \toctitle is used
%
\author{Benjamin Planche}
%
\authorrunning{Benjamin Planche} % abbreviated author list (for running head)
%
%%%% list of authors for the TOC (use if author list has to be modified)
\tocauthor{Benjamin Planche}
%
\institute{Universit\"{a}t Passau, Germany,\\
\email{planch01@stud.uni-passau.de},\\ Home page:
\texttt{https://github.com/Aldream/random-number-generator}}

\maketitle              % typeset the title of the contribution

\begin{abstract}
This survey focuses on Random Number Generators (RNGs). In this light, it first attempts to unravel the various definitions of randomness and pseudo-randomness, before detailing the different categories of random number generators, based on physical phenomena or computations. Emphasis is put on Linear- and Non-Linear Feedback Shift Registers (LFSRs and NLFSRs), and their implementation. Finally, the difficulty of objectively testing RNGs is discussed, and various test suites are described.
\keywords{RNG, randomness, entropy, pseudorandom, LFSR, NLFSR, test suite, Berlekamp-Massey algorithm}
\end{abstract}
%
\section{Introduction}
%
The generation of random numbers is an important operation for various computational tasks, but not all these applications have the same requirements about the \emph{randomness} of the obtained sequences. This paper has thus the purpose to give an overview of the most common requirements and solutions in the vast domain of random number generation.

In Section \ref{sec:rand}, we first discuss the various definitions behind randomness. Section \ref{sec:rng} describes the two main categories of RNGs and offers implementations of the famous LFSRs and NLFSRs. In Section \ref{sec:test}, we explain the problems relative to testing such generators, before presenting some famous test batteries and discussing the \emph{Berlekamp-Massey} algorithm and its implementation. Finally, we conclude this paper in Section \ref{sec:conc}.


\section{Randomness}
\label{sec:rand}


Randomness is defined as the lack of pattern, predictability or determinism in events. It is thus generally difficult to evaluate if a sequence is truly random, or if we are simply ignorant of a hidden underlying pattern.

Among other matters, information theory is studying the properties of random sequences. This notion is usually defined as being a sequence of independent random variables. However, the elaboration of a more formal and mathematical characterization is quite challenging, and has been the subject of many debates and studies during the last century.

\subsection{Definition by Von Mises}


The first attempt at defining algorithmic randomness was made by the scientist and mathematician \emph{Richard Edler von Mises} (19 April 1883 – 14 July 1953). Based on the theory of large numbers, he stated that an infinite sequence of symbols (bits actually) can be considered random if it possesses the frequency stability property, i.e. if the appearance frequencies of the symbols aren't biased and goes to the same expected value (cf. the strong law of large numbers), but also if any sub-sequence selected by a ``proper method'' isn't biased \cite{down}.

The sub-sequence criterion is fundamental in this definition. For instance, if the sequence “$10101010$” is not biased, we obtain the biased sub-sequence “$0000$” by selecting only the even positions \cite{down}.

This first definition was however unsatisfying, because of the failed attempts to mathematize what the “proper method” of selection was, and also because of a demonstration by \emph{Jean Ville} in 1939, proving that such a definition only yields an empty set \cite{down}. After the development of the information theory in the middle of the twentieth century, new definitions were proposed in the sixties, reaching some kind of consensus.

\subsection{Definition by Martin-L\"{o}f}

A sequence is random according to the Swedish statistician \emph{Per Martin-L\"{o}f} if it has no ``exceptional and effectively verifiable'' property, i.e. has no properties which can be verified by a recursive algorithm. This definition, presented in 1966 and based on the measure theory, is considered the most satisfactory notion of algorithmic randomness. It is considered as a \textit{frequency / measure-theoretic} approach to randomness \cite{down}.

\subsection{Definition by Levin/Chaitin}

\emph{Ray Solomonoff} (25 July 1926 – 7 December 2009) and \emph{Andreï Nikolaïevitch Kolmogorov} (25 April 1903 – 20 October 1987) developed in the sixties an important measure for the field of information theory: \emph{the complexity of Kolmogorov} \cite{down}. This measure is defined as the length of the shortest program (independently of the machine running it) able to generate the evaluated sequence. 


Using this work, \emph{Leonid Anatolievich Levin} (born in 1948) and \emph{Gregory Chaitin} (born in 1947) concluded in 1975 that a random finite string can be considered as a string which requires a program at least as long as itself to be computed. Another way to express it is: ``\textit{a random sequence must have an incomprehensible informational content}'', i.e. it is impossible to make any sense of it, and thus to use a shorter sequence or program to describe it \cite{down}. This is why this approach is considered as a \textit{complexity / compressibility} one.

\subsection{Definition by Schnorr}

Taking the predictability approach, \emph{Claus-Peter Schnorr} (born in 1943) used the martingales theory in 1971 to build his definition, which is the following: ``a random sequence must not be predictable. No effective strategy should lead to an infinite gain if we bet on the symbols of the sequence'' \cite{down}.



\subsection{Pseudo-Randomness}

A numeric sequence is considered statistically random if it contains no recognizable patterns or regularities. Compared to the previous definitions, statistical randomness is less strict and actually doesn't imply objective unpredictability. It thus leaves room to the concept of pseudo-randomness.

 A pseudo-random sequence exhibits statistical randomness while being generated by an entirely deterministic causal method. Such sequences are largely used in computer science, to simulate random behaviors, when generating truly random values would be too costly. Indeed, because pseudo-random sequences can be algorithmically generated, they can be used in a much simpler and frequent way.

\section{Random Number Generators}
\label{sec:rng}

Mediums to generate random sequences have been used for a long time in various domains, from politics to cryptology. However, they are not all equal, and most of them suffer from bias which can be or not a problem depending on the usages.

\subsection{Definition}

A \emph{random number generator} (RNG) is a device which can produce a sequence of random numbers, i.e. a sequence without determinist properties and patterns. Such a device can use physical interactions, computations, or a mix of both, to achieve it.

As defined before, since a purely random sequence can't be described or generated by an algorithm (not recursive), computational methods can only create pseudo-random sequences. Such generators are thus named \emph{pseudo-random number generators} (PNRG).

\subsection{Categories}

\subsubsection{Generators based on physical phenomena:}


Physical interactions, such as dice tossing or coin flipping, have been traditionally used to make random decisions and generate unpredictable sequences.

However, some physical phenomena are only random in appearances. For instance, in the case of the coin flipping, dynamics rules are applied to the trajectory of the coin. The results seem random only because we can't simply measure the variables of the toss to solve the system. But having even some meager hints about the initial conditions can then help deduce the outcome (for example, the face on top before the coin is tossed may have more chances to be the resulting one).

The best phenomena to use as input for random generators are thus the phenomena possessing \emph{quantum mechanical physical randomness}, especially found in quantum mechanics at the atomic or sub-atomic level. Based on this property, various methods have been implemented to generate random sequences: by measuring the nuclear decay with a Geiger counter, by printing “$0$” or “$1$” when a photon is reflected or transmitted by a semi-transparent mirror, etc \cite{w-rng,down}.

If these devices are seen as golden solutions, they are globally too costly to be democratized. Thermal phenomena are for instance easier to detect and offer good results. The noise obtained by amplifying the thermal signal from a transistor or from an atmospheric radio receiver are famous examples, though other solutions have also been developed (based on the comparison of pictures from an agitated scene, on the noise from analog-to-digital converter, etc.) \cite{w-rng}.

In computer science, operating systems implement various methods based on the unpredictable inputs/outputs and the behavior of the users. In Unix systems for instance, $/dev/urandom$ and $/dev/random$ are device files probing analog sources (mouse, keyboard, disk accesses, etc.) to harvest entropy and thus output random bytes; while $CryptGenRandom$ for Windows systems gather entropy though CPU counters, environment variables, threads IDs, etc. In both cases however, entropy tends to decrease during inactivity, which could lead to shortages \cite{auma}.

\subsubsection{Pseudo-Random Number Generators:}

As it has been said, PRNG are generators based on algorithms. It might seem paradoxical to associate randomness with algorithms, which are by definition deterministic. But some cleverly implemented processes can generate pseudo-random sequences with periods long enough for their usages. Because pseudo-random number generators are deterministic, their output sequences are totally defined by their initial configuration, called \emph{state}. The key variable of this state is the \emph{seed} (or \emph{random seed}), a number, or vector, which should be kept secret. Otherwise, anyone could predict the sequence generated from this setting \cite{w-rng,down}.

We present below two widespread categories of PRNGs, based on shift registers:


\paragraph{LFSR:}


A \emph{Linear Feedback Shift Register} (LFSR) is a sequential shift register whose input bit is a linear function of its previous state, i.e. with combinational logic that causes it to pseudo-randomly cycle through a sequence of binary values \cite{ray:and,joux}. A binary feedback register is thus a mapping $\mathfrak{F}:\mathbb{F}_2^n \to \mathbb{F}_2^n$, with $\mathbb{F}_2^n$ the vector space of all binary $n$-tuples, of the form:

$$\mathfrak{F}:(x_1, x_2, ..., x_n)\mapsto (x_2, x_3, ..., x_n, f((x_1, x_2, ..., x_n))$$

With $f$ the feedback function, a Boolean operation of $n$ variables, linear in the case of LFSRs. Design-speaking, this function defines the \emph{taps}, i.e. the bits of the register used in the linear operation (generally \textit{XOR}) generating the new input bit \cite{szmi,joux}.

The feedback function can also be be expressed in finite field arithmetic as a polynomial mod 2, called the feedback polynomial or reciprocal characteristic polynomial. For instance, a LFSR with taps on the \nth{16}, \nth{14}, \nth{13} and \nth{11} bits has the feedback polynomial $x_{16} + x_{14} + x_{13} + x_{11} + 1$ \cite{w-lfsr}\cite{joux}.

To obtain a maximal-length LFSR, i.e. a LFSR generating a bit sequence of period $2^n-1$, several conditions must be met by the feedback function, such as having an even number of taps or using a \textit{relatively-prime} set of taps \cite{w-lfsr}\cite{joux}.

Algorithm \ref{alg:lfsr} details a solution for the generation of linear feedback shift registers, given a set of taps and an initial value.

\begin{algorithm}[ht]
  \caption{Implementation of a generic LFSR}\label{alg:lfsr}
  \begin{algorithmic}[1]
    \Require
      \Statex $taps$: Sequence of taps defining the register (ex: $(1, 0, 0, 1)$ represents the register of feedback polynomial $x^4 + x^3 + 1$).
      \Statex $seed$: Initial value given to the register.
    \Ensure
      \Statex Sequence of values generated by the LFSR defined by the given sequence of taps and initial value.
    \Statex
    \State $degree \gets |taps|$ \Comment{Degree of the feedback polynomial}
    \State $period \gets 2^{degree} - 1$ \Comment{Max-period of the LFSR}
    \State $value \gets seed$ \Comment{Returned value}
    \State $it \gets 0$
    \While{$it < period$}
    
    	\Comment{Computing first the new value of the most-significant bit:}
        \State $bit \gets 0$
        \State $j \gets 0$
        \For{$j<degree$}
        	\If{$taps[j] \neq 0$}
            	\State $bit \gets bit \oplus (value >> j)$ \Comment{$\oplus$ representing the binary XOR-operation}
            \EndIf
            \State $j \gets j+1$
    	\EndFor
        
    	\Comment{Getting the final value in the register by popping the least-significant bit and appending the new most-significant one:}
        \State $value \gets (value >> 1) | (bit << (degree-1))$
        
        \Return $value$ \Comment{Yield the pseudo-random value}
    \EndWhile
  \end{algorithmic}
\end{algorithm}


\paragraph{NLFSR:}

The theory of \emph{Non-Linear Feedback Shift Registers} (NLFSR) are the same as for LFSRs, with the only difference than the feedback function $f$ is defined as non-linear\cite{joux}. This difference makes NLFSRs harder to predict than LFSRs, but also imposes extensive carefulness in the selection of the feedback function, in order to ensure a maximal period of $2^n-1$ bits. Conditions and lists of valid configurations can be found in \cite{dubro}.

Algorithm \ref{alg:nlfsr} shows how to generate non-linear feedback shift registers, given an initial configuration.

\begin{algorithm}[ht]
  \caption{Implementation of a generic NLFSR}\label{alg:nlfsr}
  \begin{algorithmic}[1]
    \Require
      \Statex $taps$: Sequence of combinations of taps defining the non-linear register. (ex: $([0],[1],[2],[1,2])$ represents the register of feedback polynomial $x^4 + x^3 + x^2  + x^1*x^2 + 1$).
      \Statex $seed$: Initial value given to the register.
    \Ensure
      \Statex Sequence of values generated by the NLFSR defined by the given sequence of taps and initial value.
    \Statex
    \State $degree \gets |taps|$ \Comment{Degree of the feedback polynomial}
    \State $period \gets 2^{degree} - 1$ \Comment{Max-period of the NLFSR, cf \cite{dubro}}
    \State $value \gets seed$ \Comment{Returned value}
    \State $it \gets 0$
    \While{$it < period$}
    
    	\Comment{Computing first the new value of the most-significant bit:}
        \State $bit \gets 0$
        \State $j \gets 0$
        \For{$j<degree$}
        	\If{$taps[j] \neq 0$}
            
            	\Comment{Computing the binary multiplication $x_{k_0} \otimes x_{k_1} \otimes ... \otimes x_{k_n}$ with $[k_0, k_1, ..., k_n]$ the $j$-th taps array}
        		\State $multResult \gets 1$
                \ForAll{$k \in taps[j]$}
                
                	\Comment{Binary multiplication of terms returns $1$ iif none of the terms is null. So if we encounter a null bit, we simply return $0$, else $1$.}
                	\If{$(value >> k) = 0$}
                    	\State $multResult \gets 0$
                        \State break
                    \EndIf
                \EndFor
            \Else
            	\State $multResult \gets 0$
            \EndIf
            \State $bit \gets bit \oplus multResult$ \Comment{Binary addition of the multiplication results}
            \State $j \gets j+1$
    	\EndFor
        
    	\Comment{Getting the final value in the register by popping the least-significant bit and appending the new most-significant one:}
        \State $value \gets (value >> 1) | (bit << (degree-1))$
        
        \Return $value$ \Comment{Yield the pseudo-random value}
    \EndWhile
  \end{algorithmic}
\end{algorithm}


\subsection{Applications and Uses}

Random number generators have applications in every area where unpredictable behavior is desirable or required, from cryptographic systems to gambling applications, statistical sampling, simulation or tests suites, etc. \cite{w-rng,ray:and}.

Depending on the applications, various properties can be required from the generators. For instance, a security application will need a \emph{cryptographically-secure} generator; while a shuffling method will require a generator ensuring the uniqueness of the returned values \cite{w-rng}. While cryptography and some numerical algorithms demand for \emph{qualities} mostly found in physical RNGs, there is a vast variety of computer applications which are satisfied with \emph{weaker} forms of randomness; for example to simulate random behaviors in games or to get an input for a data-integrity checksum. In those cases, PNRGs are often promoted, since they are generally faster and lighter (simple boolean logic for LFSRs and NLFSRs for instance) \cite{w-rng,ray:and}.

\section{Testing Randomness}
\label{sec:test}

As the definition of \emph{randomness} is complex and field-dependent, so are the tests designed to evaluate the quality of RNGs.
 
\subsection{About the Difficulty to Test Randomness}

As exposed in the definition of \textit{randomness} in Section \ref{sec:rand}, the term \textit{random sequence} can have various meanings depending on the field of study, making this property difficult to test. Moreover, as for cryptographic results, the large number of possibilities is generally impossible to be fully covered. For instance, testing if a random sequence has indeed \textit{no shorter construction} is impossible without checking every construction \cite{ritt}. 
This is why the randomness of a sequence is commonly analyzed through statistical tests or complexity evaluations.

Once a RNG has been developed, a battery of empirical statistical tests is run against it, in an attempt to identify statistical bias. They try to evaluate if the generated sequences follow the hypothesis of \emph{perfect behavior} (named $\mathcal{H}_0$ in \cite{lecu}), the hypothesis that the values of the sequences "\textit{ imitate independent random variables from the uniform distribution}" \cite{lecu}. Different tests will thus detect different problems by using checking various statistical behaviors. Since a full coverage is impossible, there is no universal battery of tests. It is commonly acknowledged that \textit{good} RNGs are then those which pass \textit{complicated} or \textit{numerous} tests \cite{ritt,lecu}. 


\subsection{Common Tests}


\subsubsection{DIEHARD Tests:} 

This battery of statistical tests has been developed by George Marsaglia and first published in 1995 on a downloadable CD-ROM of random numbers \cite{marsa}. It consists of fifteen tests, which are run over a large file containing the sequence, provided by the user. Those tests are: birthday spacings, overlapping permutations, ranks of 31x31 and 32x32 matrices, ranks of 6x8 matrices, monkey tests on 20-bit Words, monkey tests OPSO, OQSO, DNA, 1's count in a stream of bytes, 1's count in specific bytes, parking lot, minimum distance, random spheres, squeeze, overlapping sums, runs, and craps \cite{soto}.

\subsubsection{TestU01 Suite:}
This software library, initiated in 1985 and implemented in the ANSI C language offers a collection of utilities for the empirical statistical testing of RNGs. Among the various provided tools, it offers general implementations of the classical statistical tests for random number generators, several others proposed in the literature, and some original ones; but it also offers tools to implement specific statistical tests. \cite{lecu}.

\subsubsection{Berlekamp-Massey Algorithm:} This algorithm can't really be considered as a test. It is is used for finding in a arbitrary field $\mathbb{F}_n$ the minimal polynomial of linearly recurrent sequences, such as thoses generated by LFSRs. \emph{Elwyn Berlekamp} invented an algorithm in 1967 for decoding BCH codes \cite{berle}, then soon after \emph{James Massey} simplified and adapted it to LFSRs \cite{mass}.

The goal of the algorithm is to determine the minimal degree $L$ and the \emph{annihilator} (or \emph{inverse feedback}) polynomial $F(x)$ of the given sequence $S$, such as for all syndromes $n = L$ to $(|S|-1)$:

$$S_n + F_1 \cdot S_{n-1} + ... + F_L \cdot S_{n-L} = 0$$

Finding this minimal polynomial requires to solve a set of $L$ linear equations, $F(x)$ being thus uniquely determined by the first $2L$ elements of $S$. At each iteration $l$, the algorithm then evaluates the \emph{discrepancy} $\beta$ \cite{joux,feng:tzeng}, with:

$$ \beta = S_l + F_1 \cdot S_{l-1} + ... + F_L \cdot S_{l-L}$$
If $\beta = 0$, $F(x)$ and $L$ are still currently correct, and the algorithm can proceed to the next iteration. If $\beta \geq 0$, $F(x)$ should be concordantly adjusted, by shifting and scaling the syndromes added since the last update of $L$ \cite{joux}:

$$F(x) \gets F(x) - (\beta/\alpha) \cdot x^\delta \cdot f(x)$$


with $\delta$ the number of iterations since the last update of $L$, and $\alpha$, resp. $f(x)$, the value of $\beta$, resp. $F(x)$, before this last update.

In order to keep track of the number of errors and current degree of the polynomial, $L$ should be updated if the number $l$ of iterations is getting bigger then $2L$ (cf. above). In this case, $L$ is adjusted as follow:
$$L \gets l + 1 - L$$

A more complete definition of the process is given in Algorithm \ref{alg:ber-mas}, based on the works in \cite{joux,rodri}. A Python implementation of this algorithm has been run against the sequences generated by the previously-presented LFSR and NLFSR implementations (Algorithms \ref{alg:lfsr} and \ref{alg:nlfsr}). As expected, it was able to successfully and efficiently evaluate the annihilator polynomial of complex LFSRs (and thus their original feedback polynomial, by taking the inverse of the annihilator); and as expected it failed against the sequences generated by the non-linear registers.


\begin{algorithm}[ht]
  \caption{Implementation of the Berlekamp-Massey Algorithm over $\mathbb{F}_2$}\label{alg:ber-mas}
  \begin{algorithmic}[1]
    \Require
      \Statex $sequence$: Sequence of $\mathbb{F}_2$ elements to analyze.
    \Ensure
      \Statex Assumed inverse feedback polynomial.
    \Statex
    \Statex
    \Comment{Note: The variables names are based on those in the pseudo-code found in \cite{joux}}
    
    
    \State $N \gets |sequence|$ \Comment{Length of the sequence}
    \State $F(x) \gets 1, f(x) \gets 1$ \Comment{Polynomials, with $F$ being the supposed inverse feedback polynomial}
    \State $L \gets 0$ \Comment{Current number of assumed errors}
    \State $\delta \gets 1$ \Comment{Number of iterations since last update of $L$}
    \State $l \gets 0$
    \Comment{Computing $F(x)$ and $L$:}
    \For{$l < N$}
        \State $\beta \gets \sum_{i=0}^{L} sequence[l-i] \otimes |x^i|F$ \Comment{Evaluating the current discrepancy}
        
        \Comment{If the discrepancy is null, we can assume that $F$ and $L$ are currently correct and can continue with the next term. Else, we must adjust $F$:}
        \If{$\beta \neq 0$} \Comment{Adjusting $F$ for this term:}
            \State $g(x) \gets F(x)$ 
            \State $F(x) \gets F(x) - x^\delta f(x)$
            
            \Comment{The generic algorithm in $\mathbb{F}_q$ requires to multiply the subtractor term by $(\beta/\alpha)$ with $\alpha$ the previous non-null value of $\beta$ updated at the same time as $L$ and $f(x)$. In $\mathbb{F}_2$, this product would always be equal to $1$.}
            
            
            \If{$2 * L \leq l$}
            
            \Comment{$L$ represents the number of error, so the discrepancies will reach $0$ before $l$ grows bigger than $2*L$. If it is not the case, we must update $L$ (and thus re-initalize $\delta$), and also $f$ and $\alpha$:}
                \State $L \gets l + 1 - L$ \Comment{Number of available syndromes used to calculate discrepancies}
                \State $\delta \gets 1$
                \State $f(x) \gets g(x)$ \Comment{$f(x)$ get the previous value of $F(x)$}
            \Else
                \State $\delta \gets \delta + 1$
            \EndIf
        \Else
            \State $\delta \gets \delta + 1$
        \EndIf
     	\State $l \gets l + 1$
    \EndFor

    \Return F(x), L
    
  \end{algorithmic}
\end{algorithm}

\section{Conclusion}
\label{sec:conc}

This paper presented a short overview of the large topic which is the generation of random numbers. For more detailed explanations, please refer to the list of documents below.

%
% ---- Bibliography ----
%
\bibliographystyle{splncs}
\bibliography{rng}

\end{document}
: